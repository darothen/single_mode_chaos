{
 "metadata": {
  "name": "",
  "signature": "sha256:6c6d1f214176f74c65f0a3ce45e14eb7a1c1bb7d451309b9a448c3b98fb509c5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This script contains the logic to setup an iterated polynomial chaos expansion experiment\n",
      "using DAKOTA, which modifies some parameter in the expansion to study how the results \n",
      "produced are affected."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, pickle, shutil, time\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## SETUP THE EXPERIMENT\n",
      "exp_name = \"pcm_ols_parcel\"\n",
      "\n",
      "# Run parallel jobs?\n",
      "parallel = True # uses script interface, asynchronous instead of linked\n",
      "use_mpi  = False # runs dakota with mpich\n",
      "\n",
      "variables = [ \n",
      "    ## The set of variables about which to perform the PCE\n",
      "    # symbol, name, [prior, *parameters], meta(dictionary)\n",
      "    #['N', 'N', 1, ['uniform', 0., 10000.], 0.,{}],\n",
      "    ## ~100 - ~100000\n",
      "    #['lnN', 'lnN', 1, ['uniform', 4.6, 12.0], 4.6, {}],\n",
      "    ## ~10 - ~10000\n",
      "    ['lnN', 'lnN', 1, ['uniform', 2.3, 9.3], 2.3, {}],\n",
      "    ## 2.5 nm - 1 micron\n",
      "    #['lnmu', 'lnmu', 2, ['uniform', -6., 0.], -6., {}],\n",
      "    ['mu', 'mu', 2, ['uniform', 0., 0.8], 0., {}],\n",
      "    ['sigma', 'sigma', 3, ['uniform', 1.2, 3.0], 1.2, {}],\n",
      "    ['kappa', 'kappa', 4, ['uniform', 0.0, 1.2], 0., {}],\n",
      "    #['V', 'V', 5, ['uniform', 0.0, 10.0], 0., {}],\n",
      "    ['lnV', 'lnV', 5, ['uniform', -1.61, 2.5], -1.61, {}],\n",
      "    ['T', 'T', 6, ['uniform', 240., 300.], 200., {}],\n",
      "    ['P', 'P', 7, ['uniform', 50000., 105000.], 50000., {}],\n",
      "]\n",
      "\n",
      "responses = [ 'y', ]\n",
      "\n",
      "imports = [\n",
      "    \"import numpy as np\",\n",
      "]\n",
      "\n",
      "graphics = False\n",
      "\n",
      "function_name = \"arg\"\n",
      "function = \"\"\"\n",
      "    import numpy as np\n",
      "    import parcel_model as pm\n",
      "        \n",
      "    N = np.exp(lnN)\n",
      "    V = np.exp(lnV)\n",
      "    #mu = np.exp(lnmu)\n",
      "    \n",
      "    if not fn_toggle:\n",
      "    # Parameterization\n",
      "    #    Smax, _, _ = pm.arg2000(V, T, P, \n",
      "    #                            mus=[mu, ], sigmas=[sigma, ], Ns=[N, ], \n",
      "    #                            kappas=[kappa, ])\n",
      "    #    return np.log(Smax)\n",
      "    ## Parcel Model\n",
      "        aerosol_modes = [\n",
      "            pm.AerosolSpecies('aer', \n",
      "                               pm.Lognorm(mu=mu, sigma=sigma, N=N),\n",
      "                               kappa=kappa, bins=200),\n",
      "        ]\n",
      "        try:\n",
      "            model = pm.ParcelModel(aerosol_modes, V, T, -0.0, P, accom=0.1)\n",
      "            par_out, aer_out = model.run(t_end=1800., dt=0.01, max_steps=2000,\n",
      "                                         solver='cvode', output='dataframes',\n",
      "                                         terminate=True)\n",
      "                                         #solver_args={'time_limit': 10.})\n",
      "            Smax = par_out['S'].max()\n",
      "        except pm.ParcelModelError:\n",
      "            Smax = -1.0\n",
      "            \n",
      "        if Smax < 0: \n",
      "            #return -9999.0\n",
      "            Smax, _, _ = pm.arg2000(V, T, P, \n",
      "                                    mus=np.array([mu, ]), \n",
      "                                    sigmas=np.array([sigma, ]), \n",
      "                                    Ns=np.array([N, ]), \n",
      "                                    kappas=np.array([kappa, ]), accom=0.1)\n",
      "            print \">>>> model integration FAILED\", V, T, P, mu, sigma, kappa, N, \"|\", Smax \n",
      "\n",
      "            return np.log(Smax)\n",
      "        else:\n",
      "            return np.log(Smax)            \n",
      "    else:\n",
      "        mu *= 1e-6\n",
      "        N_act, act_frac = pm.activate_lognormal_mode(fn_toggle, \n",
      "                                                     mu, sigma, N, kappa, T=T)\n",
      "        return N_act\n",
      "    \n",
      "\"\"\"\n",
      "\n",
      "## EDIT THE DIRECTIVE \n",
      "pce_directive = \"\"\"\n",
      "    askey\n",
      "    \n",
      "    {name} = {val}\n",
      "    tensor_grid\n",
      "    collocation_ratio 4\n",
      "    basis_type total_order\n",
      "    #least_angle_regression\n",
      "    #least_absolute_shrinkage\n",
      "    least_squares\n",
      "    #cross_validation\n",
      "    \n",
      "    #p_refinement dimension_adaptive generalized\n",
      "    #max_iterations = 10\n",
      "    #convergence_tol = 1.e-8\n",
      "\n",
      "    num_probability_levels = 11\n",
      "    probability_levels = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95\n",
      "    cumulative distribution\n",
      "    \n",
      "\"\"\"\n",
      "\n",
      "param_name = \"expansion_order\"\n",
      "param_vals = [2, 3, 4, 5]\n",
      "#param_vals = [4, ]\n",
      "\n",
      "pickle.dump(dict(exp_name=exp_name, param_name=param_name, \n",
      "                 param_vals=param_vals, variables=variables,\n",
      "                 responses=responses, function_name=function_name,\n",
      "                 directive_base=pce_directive),\n",
      "            open(\"%s_exp.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"iterating parameter {name} over vals {vals}\".\\\n",
      "       format(name=param_name, vals=param_vals)\n",
      "\n",
      "results_dict = {}\n",
      "for param_val in param_vals:\n",
      "    print \"   \",  param_val\n",
      "\n",
      "    directive = pce_directive.format(name=param_name, val=param_val)\n",
      "\n",
      "    pickle.dump(dict(variables=variables, responses=responses,\n",
      "                     pce_directive=directive, imports=imports,\n",
      "                     function_name=function_name,\n",
      "                     function=function, graphics=graphics,\n",
      "                     parallel=parallel),\n",
      "                open('config.p', 'wb'))\n",
      "\n",
      "    %run gen_scripts.py\n",
      "    !chmod +x model_run.py\n",
      "\n",
      "    foldername = !(date +%Y%m%d_%H%M%S)\n",
      "\n",
      "    foldername = foldername[0]\n",
      "    print foldername\n",
      "    results_dict[\"%s_%r\" % (param_name, param_val)] = foldername\n",
      "\n",
      "    print \"Now executing simulation with DAKOTA...\",\n",
      "    if use_mpi:\n",
      "        !mpiexec -np 10 dakota -i model_pce.in > model_pce.out\n",
      "    else:\n",
      "        !dakota -i model_pce.in > model_pce.out\n",
      "    print \"complete.\\n\"\n",
      "\n",
      "    print \"Copying files...\"\n",
      "    save_dir = os.path.join(\"save\", foldername)\n",
      "    os.mkdir(save_dir)\n",
      "\n",
      "    for fn in [\"model_run.py\", \n",
      "               \"model_pce.in\", \"model_pce.out\", \"model_pce.dat\",\n",
      "               \"dakota.rst\", \"config.p\"]:\n",
      "        shutil.copy2(fn, save_dir)\n",
      "        print fn, \"->\", save_dir\n",
      "\n",
      "    print \"... done.\"\n",
      "    \n",
      "    time.sleep(1) # to ensure we don't over-write folders!\n",
      "    \n",
      "pickle.dump(results_dict, open(\"%s_results.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iterating parameter expansion_order over vals [2, 3, 4, 5]\n",
        "    2\n",
        "Writing model_run.py with model_run_script.template ...\n",
        "Writing model_pce.in with"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " model_pce_parallel.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20140904_190317\n",
        "Now executing simulation with DAKOTA...\r\n",
        "Warning: analysis_concurrency specification greater than length of\r\n",
        "\tanalysis_drivers list.  Truncating analysis_concurrency to 1.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20140904_190317\n",
        "model_pce.in -> save/20140904_190317\n",
        "model_pce.out -> save/20140904_190317\n",
        "model_pce.dat -> save/20140904_190317\n",
        "dakota.rst -> save/20140904_190317\n",
        "config.p -> save/20140904_190317\n",
        "... done.\n",
        "   "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Writing model_run.py with model_run_script.template ...\n",
        "Writing model_pce.in with model_pce_parallel.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20140904_190345\n",
        "Now executing simulation with DAKOTA...\r\n",
        "Warning: analysis_concurrency specification greater than length of\r\n",
        "\tanalysis_drivers list.  Truncating analysis_concurrency to 1.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20140904_190345\n",
        "model_pce.in -> save/20140904_190345\n",
        "model_pce.out -> save/20140904_190345\n",
        "model_pce.dat -> save/20140904_190345\n",
        "dakota.rst -> save/20140904_190345\n",
        "config.p -> save/20140904_190345\n",
        "... done.\n",
        "   "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 4\n",
        "Writing model_run.py with model_run_script.template ...\n",
        "Writing model_pce.in with model_pce_parallel.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20140904_190437\n",
        "Now executing simulation with DAKOTA...\r\n",
        "Warning: analysis_concurrency specification greater than length of\r\n",
        "\tanalysis_drivers list.  Truncating analysis_concurrency to 1.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20140904_190437\n",
        "model_pce.in -> save/20140904_190437\n",
        "model_pce.out -> save/20140904_190437\n",
        "model_pce.dat -> save/20140904_190437\n",
        "dakota.rst -> save/20140904_190437\n",
        "config.p -> save/20140904_190437\n",
        "... done.\n",
        "   "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5\n",
        "Writing model_run.py with model_run_script.template ...\n",
        "Writing model_pce.in with model_pce_parallel.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20140904_190642\n",
        "Now executing simulation with DAKOTA...\r\n",
        "Warning: analysis_concurrency specification greater than length of\r\n",
        "\tanalysis_drivers list.  Truncating analysis_concurrency to 1.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20140904_190642\n",
        "model_pce.in -> save/20140904_190642\n",
        "model_pce.out -> save/20140904_190642\n",
        "model_pce.dat -> save/20140904_190642\n",
        "dakota.rst -> save/20140904_190642\n",
        "config.p -> save/20140904_190642\n",
        "... done.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Process the output of all the runs\n",
      "results_dict = pickle.load(open(\"%s_results.dict\" % exp_name, 'rb'))\n",
      "\n",
      "for run_name, foldername in results_dict.iteritems():\n",
      "    print run_name, foldername\n",
      "    %run process_output.py {\"save/%s\" % foldername}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "expansion_order_2 20140904_190317\n",
        "expansion_order_3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20140904_190345\n",
        "expansion_order_4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20140904_190437\n",
        "expansion_order_5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 20140904_190642\n"
       ]
      }
     ],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}