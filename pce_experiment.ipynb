{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drive the probabilistic collocation method using DAKOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, pickle, shutil, sys, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "\n",
    "Define the parameters of the chaos expansion and tweak the methodology used by DAKOTA to execute it. These parameters will be saved in a folder corresponding to timestamp of when the analysis was executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_name = \"SM_LASSO\"\n",
    "\n",
    "# Run jobs or DAKOTA in parallel?\n",
    "PARALLEL = True  # if false, uses asynchronous script interface instead of linked\n",
    "USE_MPI  = False # run dakota with mpich?\n",
    "\n",
    "variables = [ \n",
    "    ## The set of variables about which to perform the PCE\n",
    "    # symbol, name, [prior, *parameters], offset, meta(dictionary)\n",
    "    ['logN', 'logN', ['uniform', 1.0, 4.0], 1.0, {}],\n",
    "    ['logmu', 'logmu', ['uniform', -3., -1.], -3., {}],\n",
    "    \n",
    "    ['sigma', 'sigma', ['uniform', 1.2, 3.0], 1.2, {}], \n",
    "    ['kappa', 'kappa', ['uniform', 0.0, 1.2], 0.0, {}],\n",
    "    \n",
    "    ['logV', 'logV', ['uniform', -2., 1.], -2., {}],\n",
    "    ['T', 'T', ['uniform', 240., 310.], 240., {}],\n",
    "    ['P', 'P', ['uniform', 50000., 105000.], 50000., {}],\n",
    "    ['accom', 'accom', ['uniform', 0.1, 1.0], 0.1, {}],\n",
    "]\n",
    "\n",
    "# Number the variables\n",
    "for i, v in enumerate(variables):\n",
    "    v.insert(2, i + 1)\n",
    "\n",
    "##############################\n",
    "## SIMULATION / DAKOTA SETUP\n",
    "\n",
    "## PARCEL MODEL SETTINGS\n",
    "\n",
    "# Name to alias the response functions\n",
    "responses = [ 'Smax', 'Neq', 'Nkn' ]\n",
    "\n",
    "# Evaluation script-specific imports\n",
    "imports = [\n",
    "    \"import numpy as np\",\n",
    "]\n",
    "\n",
    "# Function setup\n",
    "function_name = \"run_model\"\n",
    "function = \"\"\"\n",
    "    import numpy as np\n",
    "    import parcel_model as pm\n",
    "        \n",
    "    N = 10.**logN\n",
    "    mu =10.**logmu\n",
    "    V = 10.**logV\n",
    "    \n",
    "    use_param = False\n",
    "    \n",
    "    output_dt = 1.0\n",
    "    solver_dt = 10.0\n",
    "    #z_top = 1000.0\n",
    "    #t_end = np.min([1800., z_top/V])\n",
    "    t_end = 1800.\n",
    "    \n",
    "    aerosol_modes = [\n",
    "        pm.AerosolSpecies('aer', \n",
    "                           pm.Lognorm(mu=mu, sigma=sigma, N=N),\n",
    "                           kappa=kappa, bins=250),\n",
    "    ]\n",
    "    \n",
    "    if not fn_toggle:\n",
    "        if use_param:\n",
    "        ## Parameterization\n",
    "            Smax, nact, act_frac = pm.arg2000(V, T, P, \n",
    "                                              aerosol_modes, \n",
    "                                              accom=accom)\n",
    "            return np.log10(Smax), np.log10(np.sum(nact)), np.log10(np.sum(nact))\n",
    "        else:\n",
    "        ## Parcel Model\n",
    "            try:\n",
    "                model = pm.ParcelModel(aerosol_modes, V, T, -0.0, P, \n",
    "                                       accom=accom,\n",
    "                                       truncate_aerosols=True)\n",
    "                par_out, aer_out = model.run(t_end=t_end, \n",
    "                                             output_dt=output_dt,\n",
    "                                             solver_dt=solver_dt,\n",
    "                                             max_steps=2000,\n",
    "                                             solver='cvode', \n",
    "                                             output='dataframes',\n",
    "                                             terminate=True, \n",
    "                                             terminate_depth=10.)\n",
    "                Smax = par_out['S'].max()\n",
    "                T_fin = par_out['T'].iloc[-1]\n",
    "                \n",
    "                ## Compute the activated number\n",
    "                eq, kn = 0., 0.\n",
    "                for aer in aerosol_modes:\n",
    "                    rs = aer_out[aer.species].iloc[-1].values\n",
    "                    af_eq, af_kn, _, _ = pm.binned_activation(Smax, T_fin,\n",
    "                                                              rs, aer)\n",
    "                    # Use binned totals for consistency\n",
    "                    eq += af_eq*np.sum(aer.Nis*1e-6)\n",
    "                    kn += af_kn*np.sum(aer.Nis*1e-6)\n",
    "                \n",
    "            except:\n",
    "                print \">>>> (%d) model integration FAILED\" % run_id, \\\n",
    "                      \" -- \", logmu, logN, logV #, \"%r\" % e \n",
    "                Smax = -1.0\n",
    "\n",
    "            if Smax < 0: \n",
    "                #return -9999.0\n",
    "                Smax, _, _ = pm.arg2000(V, T, P, aerosol_modes, accom=accom)\n",
    "                \n",
    "                return np.log10(Smax), 0., 0.\n",
    "            else:\n",
    "                return np.log10(Smax), np.log10(eq), np.log10(kn)\n",
    "    else:\n",
    "        Smax_arg, Nact_arg, _ = pm.arg2000(V, T, P, aerosol_modes, \n",
    "                                           accom=accom)\n",
    "        Smax_arg = np.log10(Smax_arg)\n",
    "        Nact_arg = np.log10(np.sum(Nact_arg))\n",
    "        \n",
    "        Smax_mbn, Nact_mbn, _ = pm.mbn2014(V, T, P, aerosol_modes,\n",
    "                                           accom=accom)\n",
    "        Smax_mbn = np.log10(Smax_mbn)\n",
    "        Nact_mbn = np.log10(np.sum(Nact_mbn))\n",
    "        \n",
    "        Nderiv, _ = pm.lognormal_activation(fn_toggle, mu*1e-6,\n",
    "                                            sigma, N, kappa, T=T)\n",
    "        Nderiv = np.log10(Nderiv)\n",
    "                                  \n",
    "        return Smax_arg, Nact_arg, Smax_mbn, Nact_mbn, Nderiv\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "## DAKOTA SETTINGS\n",
    "graphics = False \n",
    "\n",
    "# Analysis options for DAKOTA\n",
    "pce_directive = \"\"\"\n",
    "    askey\n",
    "    \n",
    "    {name} = {val}\n",
    "    tensor_grid\n",
    "    basis_type total_order\n",
    "    \n",
    "    collocation_ratio 3\n",
    "    #least_angle_regression\n",
    "    least_absolute_shrinkage\n",
    "    #least_squares\n",
    "    \n",
    "    variance_based_decomp\n",
    "\"\"\"\n",
    "\n",
    "param_name = \"expansion_order\"\n",
    "param_vals = [2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the experiment setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(dict(exp_name=exp_name, param_name=param_name, \n",
    "                 param_vals=param_vals, variables=variables,\n",
    "                 responses=responses, function_name=function_name,\n",
    "                 directive_base=pce_directive),\n",
    "            open(\"%s_exp.dict\" % exp_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the model python and DAKOTA driver scripts and run DAKOTA\n",
    "\n",
    "These scripts will initially reside in the main directory, but will be copied to the `save` archive timestamped folder for referal later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"iterating parameter {name} over vals {vals}\".\\\n",
    "       format(name=param_name, vals=param_vals)\n",
    "\n",
    "path = sys.path\n",
    "cwd = os.getcwd()\n",
    "if not (cwd in path): sys.path.append(cwd)\n",
    "\n",
    "results_dict = {}\n",
    "for i, param_val in enumerate(param_vals):\n",
    "    print \"   \",  param_val\n",
    "\n",
    "    directive = pce_directive.format(name=param_name, val=param_val)\n",
    "\n",
    "    pickle.dump(dict(variables=variables, responses=responses,\n",
    "                     pce_directive=directive, imports=imports,\n",
    "                     function_name=function_name,\n",
    "                     function=function, graphics=graphics,\n",
    "                     parallel=PARALLEL),\n",
    "                open('config.p', 'wb'))\n",
    "\n",
    "    %run gen_scripts.py\n",
    "    !chmod +x model_run.py\n",
    "\n",
    "    foldername = !(date +%Y%m%d_%H%M%S)\n",
    "\n",
    "    foldername = foldername[0]\n",
    "    print foldername\n",
    "    results_dict[\"%s_%r\" % (param_name, param_val)] = foldername\n",
    "\n",
    "    print \"Now executing simulation with DAKOTA...\",\n",
    "    if i > 0:\n",
    "        ## Take advantage of existing restart file and cached\n",
    "        ## fn evals\n",
    "        print \"reading old restart file\"\n",
    "        if USE_MPI:\n",
    "            !mpiexec -np 10 dakota -read_restart dakota.rst.all -i model_pce.in > model_pce.out\n",
    "        else:\n",
    "            !dakota -read_restart dakota.rst.all -i model_pce.in > model_pce.out\n",
    "        print \"concatenating results into restart file\"\n",
    "        !dakota_restart_util cat dakota.rst dakota.rst.all dakota.rst.new\n",
    "        !mv dakota.rst.new dakota.rst.all\n",
    "    else:\n",
    "        if USE_MPI:\n",
    "            !mpiexec -np 10 dakota -i model_pce.in > model_pce.out\n",
    "        else:\n",
    "            !dakota -i model_pce.in > model_pce.out\n",
    "        !cp dakota.rst dakota.rst.all\n",
    "        \n",
    "    print \"complete.\\n\"\n",
    "\n",
    "    print \"Copying files...\"\n",
    "    save_dir = os.path.join(\"save\", foldername)\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "    for fn in [\"model_run.py\", \n",
    "               \"model_pce.in\", \"model_pce.out\", \"model_pce.dat\",\n",
    "               \"dakota.rst\", \"config.p\"]:\n",
    "        shutil.copy2(fn, save_dir)\n",
    "        print fn, \"->\", save_dir\n",
    "\n",
    "    print \"... done.\"\n",
    "    \n",
    "    time.sleep(1) # to ensure we don't over-write folders!\n",
    "    \n",
    "pickle.dump(results_dict, open(\"%s_results.dict\" % exp_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process output\n",
    "\n",
    "Analyze the DAKOTA output log to save the chaos expansions, sobol indices,\n",
    "etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Process the output of all the runs\n",
    "results_dict = pickle.load(open(\"%s_results.dict\" % exp_name, 'rb'))\n",
    "\n",
    "for run_name, foldername in results_dict.iteritems():\n",
    "    print run_name, foldername\n",
    "    ## Generate output for sobol, pce\n",
    "    %run process_output.py {\"save/%s\" % foldername}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling\n",
    "\n",
    "Conduct a global analysis using LHS and save the results. The cell has been re-written to automate sampling over all the chaos expansions, so only run it after all of the chaos expansions have been generated. In general, you should follow these steps:\n",
    "\n",
    "1. Re-set the IPython cluster.\n",
    "2. Run the cell once with `n = 10`\n",
    "3. Set `n` to the actual amount and run it.\n",
    "\n",
    "If you make changes to any of the utility toolkits (`dist_tools.py`, etc), you may need to reset the entire IPython kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 tasks finished after   26 s\n",
      "done\n",
      "done.\n",
      "Saving to disk at SM_LASSO_LHS_design_results.csv\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "ref_exp = \"SM_OLS\"\n",
    "project_arg = \"--project\"\n",
    "%run pce_sample --params --parcel {project_arg} -n {n} --parallel --recompute {ref_exp}\n",
    "ref_design = \"%s_LHS_design.csv\" % ref_exp\n",
    "\n",
    "## Reference to previos design available\n",
    "for exp in ['SM_LARS', 'SM_LASSO']:\n",
    "#for exp in ['SM_LASSO']:\n",
    "    new_design = \"%s_LHS_design.csv\" % exp\n",
    "    !cp {ref_design} {new_design}\n",
    "    %run pce_sample --reference {new_design} -n {n} --parallel --recompute {exp}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
