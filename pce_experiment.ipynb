{
 "metadata": {
  "name": "",
  "signature": "sha256:eec5e6ed49f361e7fcc175b40413b1e627d722a9eb0efd3243301f573178fa5e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Drive the probabilistic collocation method using DAKOTA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, pickle, shutil, time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Experiment setup\n",
      "\n",
      "Define the parameters of the chaos expansion and tweak the methodology used by DAKOTA to execute it. These parameters will be saved in a folder corresponding to timestamp of when the analysis was executed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exp_name = \"SM_OLS\"\n",
      "\n",
      "# Run jobs or DAKOTA in parallel?\n",
      "PARALLEL = False  # if false, uses asynchronous script interface instead of linked\n",
      "USE_MPI  = False # run dakota with mpich?\n",
      "\n",
      "variables = [ \n",
      "    ## The set of variables about which to perform the PCE\n",
      "    # symbol, name, [prior, *parameters], offset, meta(dictionary)\n",
      "    ['logN', 'logN', ['uniform', -1., 4.0], -1., {}],\n",
      "    ['logmu', 'logmu', ['uniform', -5., -1.], -5., {}],\n",
      "    \n",
      "    ['sigma', 'sigma', ['uniform', 1.2, 3.0], 1.2, {}], \n",
      "    ['kappa', 'kappa', ['uniform', 0.1, 0.6], 0.1, {}],\n",
      "    \n",
      "    ['logV', 'logV', ['uniform', -2., 1.], -2., {}],\n",
      "    ['T', 'T', ['uniform', 240., 310.], 240., {}],\n",
      "    ['P', 'P', ['uniform', 50000., 105000.], 50000., {}],\n",
      "    #['accom', 'accom', ['uniform', 0.1, 1.0], 0.1, {}],\n",
      "]\n",
      "\n",
      "# Number the variables\n",
      "for i, v in enumerate(variables):\n",
      "    v.insert(2, i + 1)\n",
      "\n",
      "##############################\n",
      "## SIMULATION / DAKOTA SETUP\n",
      "\n",
      "## PARCEL MODEL SETTINGS\n",
      "\n",
      "# Name to alias the response functions\n",
      "responses = [ 'Smax', 'Neq', 'Nkn' ]\n",
      "\n",
      "# Evaluation script-specific imports\n",
      "imports = [\n",
      "    \"import numpy as np\",\n",
      "]\n",
      "\n",
      "# Function setup\n",
      "function_name = \"run_model\"\n",
      "function = \"\"\"\n",
      "    import numpy as np\n",
      "    import parcel_model as pm\n",
      "        \n",
      "    N = 10.**logN\n",
      "    mu =10.**logmu\n",
      "    V = 10.**logV\n",
      "    accom = 0.1\n",
      "    \n",
      "    use_param = True\n",
      "    \n",
      "    aerosol_modes = [\n",
      "        pm.AerosolSpecies('aer', \n",
      "                           pm.Lognorm(mu=mu, sigma=sigma, N=N),\n",
      "                           kappa=kappa, bins=250),\n",
      "    ]\n",
      "    \n",
      "    if not fn_toggle:\n",
      "        if use_param:\n",
      "        ## Parameterization\n",
      "            Smax, nact, act_frac = pm.arg2000(V, T, P, \n",
      "                                              aerosol_modes, \n",
      "                                              accom=accom)\n",
      "            return np.log10(Smax), np.log10(np.sum(nact)), np.log10(np.sum(nact))\n",
      "        else:\n",
      "        ## Parcel Model\n",
      "            try:\n",
      "                model = pm.ParcelModel(aerosol_modes, V, T, -0.0, P, \n",
      "                                       accom=accom,\n",
      "                                       truncate_aerosols=True)\n",
      "                par_out, aer_out = model.run(t_end=1800., dt=0.01, \n",
      "                                             max_steps=2000,\n",
      "                                             solver='cvode', \n",
      "                                             output='dataframes',\n",
      "                                             terminate=True)\n",
      "                Smax = par_out['S'].max()\n",
      "                T_fin = par_out['T'].iloc[-1]\n",
      "                \n",
      "                ## Compute the activated number\n",
      "                eq, kn = 0., 0.\n",
      "                for aer in aerosol_modes:\n",
      "                    rs = aer_out[aer.species].iloc[-1].values\n",
      "                    af_eq, af_kn, _, _ = pm.binned_activation(Smax, T_fin,\n",
      "                                                              rs, aer)\n",
      "                    # Use binned totals for consistency\n",
      "                    eq += af_eq*np.sum(aer.Nis*1e-6)\n",
      "                    kn += af_kn*np.sum(aer.Nis*1e-6)\n",
      "                \n",
      "            except pm.ParcelModelError as e:\n",
      "                print \">>>> (%d) model integration FAILED\" % run_id, \\\n",
      "                      \" -- \", mu_MOS, mu_MBS, \"%r\" % e \n",
      "                Smax = -1.0\n",
      "\n",
      "            if Smax < 0: \n",
      "                #return -9999.0\n",
      "                Smax, _, _ = pm.arg2000(V, T, P, aerosol_modes, accom=accom)\n",
      "                \n",
      "                return np.log10(Smax), 0., 0.\n",
      "            else:\n",
      "                return np.log10(Smax), np.log10(eq), np.log10(kn)\n",
      "    else:\n",
      "        Smax_arg, Nact_arg, _ = pm.arg2000(V, T, P, aerosol_modes, \n",
      "                                           accom=accom)\n",
      "        Smax_arg = np.log10(Smax_arg)\n",
      "        Nact_arg = np.log10(np.sum(Nact_arg))\n",
      "        \n",
      "        Smax_mbn, Nact_mbn, _ = pm.mbn2014(V, T, P, aerosol_modes,\n",
      "                                           accom=accom)\n",
      "        Smax_mbn = np.log10(Smax_mbn)\n",
      "        Nact_mbn = np.log10(np.sum(Nact_mbn))\n",
      "        \n",
      "        Nderiv, _ = pm.lognormal_activation(fn_toggle, mu*1e-6,\n",
      "                                            sigma, N, kappa, T=T)\n",
      "        Nderiv = np.log10(Nderiv)\n",
      "                                  \n",
      "        return Smax_arg, Nact_arg, Smax_mbn, Nact_mbn, Nderiv\n",
      "\"\"\"\n",
      "    \n",
      "\n",
      "## DAKOTA SETTINGS\n",
      "graphics = False \n",
      "\n",
      "# Analysis options for DAKOTA\n",
      "pce_directive = \"\"\"\n",
      "    askey\n",
      "    \n",
      "    {name} = {val}\n",
      "    tensor_grid\n",
      "    basis_type total_order\n",
      "    \n",
      "    collocation_ratio 3\n",
      "    #least_angle_regression\n",
      "    #least_absolute_shrinkage\n",
      "    least_squares\n",
      "    \n",
      "    variance_based_decomp\n",
      "\"\"\"\n",
      "\n",
      "param_name = \"expansion_order\"\n",
      "param_vals = [2, 3 ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the experiment setup."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(dict(exp_name=exp_name, param_name=param_name, \n",
      "                 param_vals=param_vals, variables=variables,\n",
      "                 responses=responses, function_name=function_name,\n",
      "                 directive_base=pce_directive),\n",
      "            open(\"%s_exp.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate the model python and DAKOTA driver scripts and run DAKOTA\n",
      "\n",
      "These scripts will initially reside in the main directory, but will be copied to the `save` archive timestamped folder for referal later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"iterating parameter {name} over vals {vals}\".\\\n",
      "       format(name=param_name, vals=param_vals)\n",
      "\n",
      "path = sys.path\n",
      "cwd = os.getcwd()\n",
      "if not (cwd in path): sys.path.append(cwd)\n",
      "\n",
      "results_dict = {}\n",
      "for i, param_val in enumerate(param_vals):\n",
      "    print \"   \",  param_val\n",
      "\n",
      "    directive = pce_directive.format(name=param_name, val=param_val)\n",
      "\n",
      "    pickle.dump(dict(variables=variables, responses=responses,\n",
      "                     pce_directive=directive, imports=imports,\n",
      "                     function_name=function_name,\n",
      "                     function=function, graphics=graphics,\n",
      "                     parallel=PARALLEL),\n",
      "                open('config.p', 'wb'))\n",
      "\n",
      "    %run gen_scripts.py\n",
      "    !chmod +x model_run.py\n",
      "\n",
      "    foldername = !(date +%Y%m%d_%H%M%S)\n",
      "\n",
      "    foldername = foldername[0]\n",
      "    print foldername\n",
      "    results_dict[\"%s_%r\" % (param_name, param_val)] = foldername\n",
      "\n",
      "    print \"Now executing simulation with DAKOTA...\",\n",
      "    if i > 0:\n",
      "        ## Take advantage of existing restart file and cached\n",
      "        ## fn evals\n",
      "        print \"reading old restart file\"\n",
      "        if USE_MPI:\n",
      "            !mpiexec -np 10 dakota -read_restart dakota.rst.all -i model_pce.in > model_pce.out\n",
      "        else:\n",
      "            !dakota -read_restart dakota.rst.all -i model_pce.in > model_pce.out\n",
      "        print \"concatenating results into restart file\"\n",
      "        !dakota_restart_util cat dakota.rst dakota.rst.all dakota.rst.new\n",
      "        !mv dakota.rst.new dakota.rst.all\n",
      "    else:\n",
      "        if USE_MPI:\n",
      "            !mpiexec -np 10 dakota -i model_pce.in > model_pce.out\n",
      "        else:\n",
      "            !dakota -i model_pce.in > model_pce.out\n",
      "        !cp dakota.rst dakota.rst.all\n",
      "        \n",
      "    print \"complete.\\n\"\n",
      "\n",
      "    print \"Copying files...\"\n",
      "    save_dir = os.path.join(\"save\", foldername)\n",
      "    os.mkdir(save_dir)\n",
      "\n",
      "    for fn in [\"model_run.py\", \n",
      "               \"model_pce.in\", \"model_pce.out\", \"model_pce.dat\",\n",
      "               \"dakota.rst\", \"config.p\"]:\n",
      "        shutil.copy2(fn, save_dir)\n",
      "        print fn, \"->\", save_dir\n",
      "\n",
      "    print \"... done.\"\n",
      "    \n",
      "    time.sleep(1) # to ensure we don't over-write folders!\n",
      "    \n",
      "pickle.dump(results_dict, open(\"%s_results.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iterating parameter expansion_order over vals [2, 3]\n",
        "    2\n",
        "Writing model_run.py with model_run_linked.template ...\n",
        "Writing model_pce.in with model_pce.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20141104_173207\n",
        "Now executing simulation with DAKOTA..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20141104_173207\n",
        "model_pce.in -> save/20141104_173207\n",
        "model_pce.out -> save/20141104_173207\n",
        "model_pce.dat -> save/20141104_173207\n",
        "dakota.rst -> save/20141104_173207\n",
        "config.p -> save/20141104_173207\n",
        "... done.\n",
        "   "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3\n",
        "Writing model_run.py with model_run_linked.template ...\n",
        "Writing model_pce.in with model_pce.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20141104_173210\n",
        "Now executing simulation with DAKOTA... reading old restart file\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "concatenating results into restart file\n",
        "Writing new restart file dakota.rst.new\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "dakota.rst processing completed: 468 evaluations retrieved.\r\n",
        "dakota.rst.all processing completed: 108 evaluations retrieved.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20141104_173210\n",
        "model_pce.in -> save/20141104_173210\n",
        "model_pce.out -> save/20141104_173210\n",
        "model_pce.dat -> save/20141104_173210\n",
        "dakota.rst -> save/20141104_173210\n",
        "config.p -> save/20141104_173210\n",
        "... done.\n"
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Process output\n",
      "\n",
      "Analyze the DAKOTA output log to save the chaos expansions, sobol indices,\n",
      "etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Process the output of all the runs\n",
      "results_dict = pickle.load(open(\"%s_results.dict\" % exp_name, 'rb'))\n",
      "\n",
      "for run_name, foldername in results_dict.iteritems():\n",
      "    print run_name, foldername\n",
      "    ## Generate output for sobol, pce\n",
      "    %run process_output.py {\"save/%s\" % foldername}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "expansion_order_2 20141104_173207\n",
        "Extracting chaos expansion coefficients...\n",
        "    Smax\n",
        "    FOUND START 2461\n",
        "    FOUND END 2499\n",
        "    Coefficients are between 2461 and 2499\n",
        "    Neq\n",
        "    FOUND START 2500\n",
        "    FOUND END 2538\n",
        "    Coefficients are between 2500 and 2538\n",
        "    Nkn\n",
        "    FOUND START 2539\n",
        "    FOUND END 2577\n",
        "    Coefficients are between 2539 and 2577\n",
        "writing to save/20141104_173207/pce.p"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Extracting Sobol indices...\n",
        "Sobol indices begin at  2602\n",
        "    Smax\n",
        "Smax -> 2 31\n",
        "    Neq\n",
        "Neq -> 2 31\n",
        "    Nkn\n",
        "Nkn -> 2 31\n",
        "writing to save/20141104_173207/sobol.p\n",
        "expansion_order_3 20141104_173210\n",
        "Extracting chaos expansion coefficients...\n",
        "    Smax\n",
        "    FOUND START 10166\n",
        "    FOUND END 10288\n",
        "    Coefficients are between 10166 and 10288\n",
        "   "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Neq\n",
        "    FOUND START 10289\n",
        "    FOUND END 10411\n",
        "    Coefficients are between 10289 and 10411\n",
        "    Nkn\n",
        "    FOUND START 10412\n",
        "    FOUND END 10534\n",
        "    Coefficients are between 10412 and 10534\n",
        "writing to save/20141104_173210/pce.p"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Extracting Sobol indices...\n",
        "Sobol indices begin at  10559\n",
        "    Smax\n",
        "Smax -> 2 66\n",
        "    Neq\n",
        "Neq -> 2 66\n",
        "    Nkn\n",
        "Nkn -> 2 66\n",
        "writing to save/20141104_173210/sobol.p\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sampling\n",
      "\n",
      "Conduct a global analysis using LHS and save the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run pce_sample --params -n 1000 --parallel --recompute {exp_name}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1000/1000 tasks finished after    2 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "done\n",
        "done.\n",
        "Saving to disk at SM_OLS_LHS_design_results.csv\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}