{
 "metadata": {
  "name": "",
  "signature": "sha256:98184b38cfb295d87b2ce864c6ad722d740bfafcdde9374c647e37e35d379cbf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Drive the probabilistic collocation method using DAKOTA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, pickle, shutil, time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Experiment setup\n",
      "\n",
      "Define the parameters of the chaos expansion and tweak the methodology used by DAKOTA to execute it. These parameters will be saved in a folder corresponding to timestamp of when the analysis was executed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exp_name = \"MARC_4var\"\n",
      "\n",
      "# Run jobs or DAKOTA in parallel?\n",
      "PARALLEL = True  # if false, uses asynchronous script interface instead of linked\n",
      "USE_MPI  = False # run dakota with mpich?\n",
      "\n",
      "variables = [ \n",
      "    ## The set of variables about which to perform the PCE\n",
      "    # symbol, name, [prior, *parameters], offset, meta(dictionary)\n",
      "    ['logN_AIT', 'logN_AIT', ['uniform', -1., 5.0], -1., {}],\n",
      "    ['logN_ACC', 'logN_ACC', ['uniform', -1., 5.0], -1., {}],\n",
      "    ['logN_MOS', 'logN_MOS', ['uniform', -1., 5.0], -1., {}],\n",
      "    ['logN_MBS', 'logN_MBS', ['uniform', -1., 5.0], -1., {}],\n",
      "\n",
      "    ['logmu_AIT', 'logmu_AIT', ['uniform', -5., -1.], -5., {}],\n",
      "    ['logmu_ACC', 'logmu_ACC', ['uniform', -3.,  0.], -3., {}],\n",
      "    ['logmu_MOS', 'logmu_MOS', ['uniform', -4., -1.], -4., {}],\n",
      "    ['logmu_MBS', 'logmu_MBS', ['uniform', -4., -1.], -4., {}],\n",
      "    \n",
      "    ['kappa_MOS', 'kappa_MOS', ['uniform', 0.1, 0.6], 0.1, {}],\n",
      "    \n",
      "    ['logV', 'logV', ['uniform', -2., 1.], -2., {}],\n",
      "    ['T', 'T', ['uniform', 240., 310.], 240., {}],\n",
      "    ['P', 'P', ['uniform', 50000., 105000.], 50000., {}],\n",
      "]\n",
      "\n",
      "# Number the variables\n",
      "for i, v in enumerate(variables):\n",
      "    v.insert(2, i + 1)\n",
      "\n",
      "##############################\n",
      "## SIMULATION / DAKOTA SETUP\n",
      "\n",
      "## PARCEL MODEL SETTINGS\n",
      "\n",
      "# Name to alias the response functions\n",
      "responses = [ 'Smax', 'Neq', 'Nkn' ]\n",
      "\n",
      "# Evaluation script-specific imports\n",
      "imports = [\n",
      "    \"import numpy as np\",\n",
      "]\n",
      "\n",
      "# Function setup\n",
      "function_name = \"run_model\"\n",
      "function = \"\"\"\n",
      "    import numpy as np\n",
      "    import parcel_model as pm\n",
      "        \n",
      "    N_AIT = 10.**(logN_AIT)\n",
      "    N_ACC = 10.**(logN_ACC)\n",
      "    N_MOS = 10.**(logN_MOS)\n",
      "    N_MBS = 10.**(logN_MBS)\n",
      "    \n",
      "    mu_AIT = 10.**(logmu_AIT)\n",
      "    mu_ACC = 10.**(logmu_ACC)\n",
      "    mu_MOS = 10.**(logmu_MOS)\n",
      "    mu_MBS = 10.**(logmu_MBS)\n",
      "    \n",
      "    V = 10.**logV\n",
      "    \n",
      "    use_param = False\n",
      "    \n",
      "    aerosol_modes = [\n",
      "        pm.AerosolSpecies('AIT', \n",
      "                           pm.Lognorm(mu=mu_AIT, sigma=1.59, N=N_AIT),\n",
      "                           kappa=0.507, bins=100),\n",
      "        pm.AerosolSpecies('ACC', \n",
      "                           pm.Lognorm(mu=mu_ACC, sigma=1.59, N=N_ACC),\n",
      "                           kappa=0.507, bins=200),\n",
      "        pm.AerosolSpecies('MOS', \n",
      "                           pm.Lognorm(mu=mu_MOS, sigma=2.0, N=N_MOS),\n",
      "                           kappa=kappa_MOS, bins=100),\n",
      "        pm.AerosolSpecies('MBS', \n",
      "                           pm.Lognorm(mu=mu_MBS, sigma=2.0, N=N_MBS),\n",
      "                           kappa=0.507, bins=100),\n",
      "    ]\n",
      "    \n",
      "    if not fn_toggle:\n",
      "        if use_param:\n",
      "            ## Parameterization\n",
      "            Smax, nact, act_frac = pm.arg2000(V, T, P, \n",
      "                                              aerosol_modes, \n",
      "                                              accom=0.1)\n",
      "            return np.log10(Smax), np.sum(nact), np.sum(nact)\n",
      "        else:\n",
      "        ## Parcel Model\n",
      "            try:\n",
      "                model = pm.ParcelModel(aerosol_modes, V, T, -0.0, P, \n",
      "                                       accom=0.1,\n",
      "                                       truncate_aerosols=True)\n",
      "                par_out, aer_out = model.run(t_end=1800., dt=0.01, \n",
      "                                             max_steps=2000,\n",
      "                                             solver='cvode', \n",
      "                                             output='dataframes',\n",
      "                                             terminate=True)\n",
      "                Smax = par_out['S'].max()\n",
      "                T_fin = par_out['T'].iloc[-1]\n",
      "                \n",
      "                ## Compute the activated number\n",
      "                eq, kn = 0., 0.\n",
      "                for aer in aerosol_modes:\n",
      "                    rs = aer_out[aer.species].iloc[-1].values\n",
      "                    af_eq, af_kn, _, _ = pm.binned_activation(Smax, T_fin,\n",
      "                                                              rs, aer)\n",
      "                    # Use binned totals for consistency\n",
      "                    eq += af_eq*np.sum(aer.Nis*1e-6)\n",
      "                    kn += af_kn*np.sum(aer.Nis*1e-6)\n",
      "                \n",
      "            except pm.ParcelModelError as e:\n",
      "                print \">>>> (%d) model integration FAILED\" % run_id, \\\n",
      "                      \" -- \", mu_MOS, mu_MBS, \"%r\" % e \n",
      "                Smax = -1.0\n",
      "\n",
      "            if Smax < 0: \n",
      "                #return -9999.0\n",
      "                Smax, _, _ = pm.arg2000(V, T, P, aerosol_modes, accom=0.1)\n",
      "                \n",
      "                return np.log10(Smax), 0., 0.\n",
      "            else:\n",
      "                return np.log10(Smax), eq, kn\n",
      "    else:\n",
      "        Smax_arg, Nact_arg, _ = pm.arg2000(V, T, P, aerosol_modes, \n",
      "                                           accom=0.1)\n",
      "        Smax_arg = np.log10(Smax_arg)\n",
      "        Nact_arg = np.sum(Nact_arg)\n",
      "        \n",
      "        Smax_mbn, Nact_mbn, _ = pm.mbn2014(V, T, P, aerosol_modes,\n",
      "                                           accom=0.1)\n",
      "        Smax_mbn = np.log10(Smax_mbn)\n",
      "        Nact_mbn = np.sum(Nact_mbn)\n",
      "                                  \n",
      "        return Smax_arg, Nact_arg, Smax_mbn, Nact_mbn\n",
      "\"\"\"\n",
      "    \n",
      "\n",
      "## DAKOTA SETTINGS\n",
      "graphics = False \n",
      "\n",
      "# Analysis options for DAKOTA\n",
      "pce_directive = \"\"\"\n",
      "    askey\n",
      "    \n",
      "    ## Use probabalistic collocation method,\n",
      "    {name} = {val}\n",
      "    tensor_grid\n",
      "    ## with all cross-terms\n",
      "    basis_type total_order\n",
      "    \n",
      "    ## over-sampling ratio \n",
      "    collocation_ratio 3\n",
      "    ## how to compute coefficients\n",
      "    #least_angle_regression\n",
      "    #least_absolute_shrinkage\n",
      "    least_squares\n",
      "    \n",
      "    #cross_validation\n",
      "    \n",
      "    # post-processing\n",
      "    #num_probability_levels = 11\n",
      "    #probability_levels = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95\n",
      "    #cumulative distribution\n",
      "    \n",
      "\"\"\"\n",
      "\n",
      "param_name = \"expansion_order\"\n",
      "param_vals = [1, ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the experiment setup."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(dict(exp_name=exp_name, param_name=param_name, \n",
      "                 param_vals=param_vals, variables=variables,\n",
      "                 responses=responses, function_name=function_name,\n",
      "                 directive_base=pce_directive),\n",
      "            open(\"%s_exp.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate the model python and DAKOTA driver scripts and run DAKOTA\n",
      "\n",
      "These scripts will initially reside in the main directory, but will be copied to the `save` archive timestamped folder for referal later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"iterating parameter {name} over vals {vals}\".\\\n",
      "       format(name=param_name, vals=param_vals)\n",
      "\n",
      "path = sys.path\n",
      "cwd = os.getcwd()\n",
      "if not (cwd in path): sys.path.append(cwd)\n",
      "\n",
      "results_dict = {}\n",
      "for i, param_val in enumerate(param_vals):\n",
      "    print \"   \",  param_val\n",
      "\n",
      "    directive = pce_directive.format(name=param_name, val=param_val)\n",
      "\n",
      "    pickle.dump(dict(variables=variables, responses=responses,\n",
      "                     pce_directive=directive, imports=imports,\n",
      "                     function_name=function_name,\n",
      "                     function=function, graphics=graphics,\n",
      "                     parallel=PARALLEL),\n",
      "                open('config.p', 'wb'))\n",
      "\n",
      "    %run gen_scripts.py\n",
      "    !chmod +x model_run.py\n",
      "\n",
      "    foldername = !(date +%Y%m%d_%H%M%S)\n",
      "\n",
      "    foldername = foldername[0]\n",
      "    print foldername\n",
      "    results_dict[\"%s_%r\" % (param_name, param_val)] = foldername\n",
      "\n",
      "    print \"Now executing simulation with DAKOTA...\",\n",
      "    if i > 0:\n",
      "        ## Take advantage of existing restart file and cached\n",
      "        ## fn evals\n",
      "        print \"reading old restart file\"\n",
      "        if USE_MPI:\n",
      "            !mpiexec -np 10 dakota -read_restart dakota.rst.all -i model_pce.in > model_pce.out\n",
      "        else:\n",
      "            !dakota -read_restart dakota.rst.all -i model_pce.in > model_pce.out\n",
      "        print \"concatenating results into restart file\"\n",
      "        !dakota_restart_util cat dakota.rst dakota.rst.all dakota.rst.new\n",
      "        !mv dakota.rst.new dakota.rst.all\n",
      "    else:\n",
      "        if USE_MPI:\n",
      "            !mpiexec -np 10 dakota -i model_pce.in > model_pce.out\n",
      "        else:\n",
      "            !dakota -i model_pce.in > model_pce.out\n",
      "        !cp dakota.rst dakota.rst.all\n",
      "        \n",
      "    print \"complete.\\n\"\n",
      "\n",
      "    print \"Copying files...\"\n",
      "    save_dir = os.path.join(\"save\", foldername)\n",
      "    os.mkdir(save_dir)\n",
      "\n",
      "    for fn in [\"model_run.py\", \n",
      "               \"model_pce.in\", \"model_pce.out\", \"model_pce.dat\",\n",
      "               \"dakota.rst\", \"config.p\"]:\n",
      "        shutil.copy2(fn, save_dir)\n",
      "        print fn, \"->\", save_dir\n",
      "\n",
      "    print \"... done.\"\n",
      "    \n",
      "    time.sleep(1) # to ensure we don't over-write folders!\n",
      "    \n",
      "pickle.dump(results_dict, open(\"%s_results.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iterating parameter expansion_order over vals [1]\n",
        "    1\n",
        "Writing model_run.py with model_run_script.template ...\n",
        "Writing model_pce.in with model_pce_parallel.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20141104_121517\n",
        "Now executing simulation with DAKOTA..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "Warning: analysis_concurrency specification greater than length of\r\n",
        "\tanalysis_drivers list.  Truncating analysis_concurrency to 1.\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/bin/sh: line 1:  8184 Bus error: 10           dakota -i model_pce.in > model_pce.out\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " complete.\n",
        "\n",
        "Copying files...\n",
        "model_run.py -> save/20141104_121517\n",
        "model_pce.in -> save/20141104_121517\n",
        "model_pce.out -> save/20141104_121517\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "[Errno 2] No such file or directory: 'model_pce.dat'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-04f6bbeee5a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                \u001b[0;34m\"model_pce.in\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_pce.out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_pce.dat\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                \"dakota.rst\", \"config.p\"]:\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"->\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/daniel/anaconda/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mcopy2\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/daniel/anaconda/lib/python2.7/shutil.pyc\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSpecialFileError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"`%s` is a named pipe\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'model_pce.dat'"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Process output\n",
      "\n",
      "Analyze the DAKOTA output log to save the chaos expansions, sobol indices,\n",
      "etc."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Process the output of all the runs\n",
      "results_dict = pickle.load(open(\"%s_results.dict\" % exp_name, 'rb'))\n",
      "\n",
      "for run_name, foldername in results_dict.iteritems():\n",
      "    print run_name, foldername\n",
      "    ## Generate output for sobol, pce\n",
      "    %run process_output.py {\"save/%s\" % foldername}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Sampling\n",
      "\n",
      "Conduct a global analysis using LHS and save the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run pce_sample --params -n 10000 --parallel {exp_name}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}