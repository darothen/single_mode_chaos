{
 "metadata": {
  "name": "",
  "signature": "sha256:72556c1757be66cefb9f3ac283a44a096a3c86b5ae3faec6f7b53844079a85d9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Drive the probabilistic collocation method using DAKOTA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, pickle, shutil, time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Experiment setup\n",
      "\n",
      "Define the parameters of the chaos expansion and tweak the methodology used by DAKOTA to execute it. These parameters will be saved in a folder corresponding to timestamp of when the analysis was executed."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "exp_name = \"MARC_4var\"\n",
      "\n",
      "# Run jobs or DAKOTA in parallel?\n",
      "PARALLEL = True  # if false, uses asynchronous script interface instead of linked\n",
      "USE_MPI  = False # run dakota with mpich?\n",
      "\n",
      "variables = [ \n",
      "    ## The set of variables about which to perform the PCE\n",
      "    # symbol, name, [prior, *parameters], offset, meta(dictionary)\n",
      "    ['logN_AIT', 'logN_AIT', ['uniform', -1., 5.0], -1., {}],\n",
      "    ['logN_ACC', 'logN_ACC', ['uniform', -1., 5.0], -1., {}],\n",
      "    ['logN_MOS', 'logN_MOS', ['uniform', -1., 5.0], -1., {}],\n",
      "    ['logN_MBS', 'logN_MBS', ['uniform', -1., 5.0], -1., {}],\n",
      "\n",
      "    ['logmu_AIT', 'logmu_AIT', ['uniform', -5., -1.], -5., {}],\n",
      "    ['logmu_ACC', 'logmu_ACC', ['uniform', -3.,  0.], -3., {}],\n",
      "    ['logmu_MOS', 'logmu_MOS', ['uniform', -4., -1.], -4., {}],\n",
      "    ['logmu_MBS', 'logmu_MBS', ['uniform', -4., -1.], -4., {}],\n",
      "    \n",
      "    ['kappa_MOS', 'kappa_MOS', ['uniform', 0.1, 0.6], 0.1, {}],\n",
      "    \n",
      "    ['logV', 'logV', ['uniform', -2., 1.], -2., {}],\n",
      "    ['T', 'T', ['uniform', 240., 310.], 240., {}],\n",
      "    ['P', 'P', ['uniform', 50000., 105000.], 50000., {}],\n",
      "]\n",
      "\n",
      "# Number the variables\n",
      "for i, v in enumerate(variables):\n",
      "    v.insert(2, i + 1)\n",
      "\n",
      "##############################\n",
      "## SIMULATION / DAKOTA SETUP\n",
      "\n",
      "## PARCEL MODEL SETTINGS\n",
      "\n",
      "# Name to alias the response functions\n",
      "responses = [ 'Smax', 'Nact_eq', 'Nact_kn' ]\n",
      "\n",
      "# Evaluation script-specific imports\n",
      "imports = [\n",
      "    \"import numpy as np\",\n",
      "]\n",
      "\n",
      "# Function setup\n",
      "function_name = \"run_model\"\n",
      "function = \"\"\"\n",
      "    import numpy as np\n",
      "    import parcel_model as pm\n",
      "        \n",
      "    N_AIT = 10.**(logN_AIT)\n",
      "    N_ACC = 10.**(logN_ACC)\n",
      "    N_MOS = 10.**(logN_MOS)\n",
      "    N_MBS = 10.**(logN_MBS)\n",
      "    \n",
      "    mu_AIT = 10.**(logmu_AIT)\n",
      "    mu_ACC = 10.**(logmu_ACC)\n",
      "    mu_MOS = 10.**(logmu_MOS)\n",
      "    mu_MBS = 10.**(logmu_MBS)\n",
      "    \n",
      "    V = 10.**logV\n",
      "    \n",
      "    use_param = False\n",
      "    \n",
      "    aerosol_modes = [\n",
      "        pm.AerosolSpecies('AIT', \n",
      "                           pm.Lognorm(mu=mu_AIT, sigma=1.59, N=N_AIT),\n",
      "                           kappa=0.507, bins=100),\n",
      "        pm.AerosolSpecies('ACC', \n",
      "                           pm.Lognorm(mu=mu_ACC, sigma=1.59, N=N_ACC),\n",
      "                           kappa=0.507, bins=200),\n",
      "        pm.AerosolSpecies('MOS', \n",
      "                           pm.Lognorm(mu=mu_MOS, sigma=2.0, N=N_MOS),\n",
      "                           kappa=kappa_MOS, bins=100),\n",
      "        pm.AerosolSpecies('MBS', \n",
      "                           pm.Lognorm(mu=mu_MBS, sigma=2.0, N=N_MBS),\n",
      "                           kappa=0.507, bins=100),\n",
      "    ]\n",
      "    \n",
      "    if not fn_toggle:\n",
      "        if use_param:\n",
      "            ## Parameterization\n",
      "            Smax, nact, act_frac = pm.arg2000(V, T, P, \n",
      "                                              aerosol_modes, \n",
      "                                              accom=0.1)\n",
      "            return np.log10(Smax), nact, nact\n",
      "        else:\n",
      "        ## Parcel Model\n",
      "            try:\n",
      "                model = pm.ParcelModel(aerosol_modes, V, T, -0.0, P, \n",
      "                                       accom=0.1,\n",
      "                                       truncate_aerosols=True)\n",
      "                par_out, aer_out = model.run(t_end=1800., dt=0.01, \n",
      "                                             max_steps=2000,\n",
      "                                             solver='cvode', \n",
      "                                             output='dataframes',\n",
      "                                             terminate=True)\n",
      "                Smax = par_out['S'].max()\n",
      "                T_fin = par_out['T'].iloc[-1]\n",
      "                \n",
      "                ## Compute the activated number\n",
      "                eq, kn = 0., 0.\n",
      "                for aer in aerosol_modes:\n",
      "                    rs = aer_out[aer.species].iloc[-1].values\n",
      "                    af_eq, af_kn, _, _ = pm.binned_activatio(Smax, T_fin,\n",
      "                                                             rs, aerosol)\n",
      "                    # Use binned totals for consistency\n",
      "                    eq += af_eq*np.sum(aer.Nis*1e-6)\n",
      "                    kn += af_kn*np.sum(aer.Nis*1e-6)\n",
      "                \n",
      "            except pm.ParcelModelError as e:\n",
      "                print \">>>> (%d) model integration FAILED\" % run_id, \\\n",
      "                      \" -- \", mu_MOS, mu_MBS, \"%r\" % e \n",
      "                Smax = -1.0\n",
      "\n",
      "            if Smax < 0: \n",
      "                #return -9999.0\n",
      "                Smax, _, _ = pm.arg2000(V, T, P, aerosol_modes, accom=0.1)\n",
      "                \n",
      "                return np.log10(Smax), 0., 0.\n",
      "            else:\n",
      "                return np.log10(Smax), eq, kn\n",
      "    else:\n",
      "        smax = fn_toggle\n",
      "    \n",
      "        mus = np.array([mu_ACC, mu_MOS, mu_MBS])*1e-6\n",
      "        Ns = np.array([N_ACC, N_MOS, N_MBS])\n",
      "        sigmas = np.array([1.59, 2.0, 2.0])\n",
      "        kappas = np.array([0.507, kappa_MOS, 0.507])\n",
      "        \n",
      "        n_acts, act_fracs = [], []\n",
      "        for mu, sigma, N, kappa in zip(mus, sigmas, Ns, kappas):\n",
      "            N_act, act_frac = pm.lognormal_activation(smax, mu, \n",
      "                                                      sigma, N, kappa, T=T)\n",
      "            n_acts.append(N_act)\n",
      "            act_fracs.append(act_frac)\n",
      "        N_act_final = np.sum(n_acts)\n",
      "        return N_act_final\n",
      "\"\"\"\n",
      "    \n",
      "\n",
      "## DAKOTA SETTINGS\n",
      "graphics = False \n",
      "\n",
      "# Analysis options for DAKOTA\n",
      "pce_directive = \"\"\"\n",
      "    askey\n",
      "    \n",
      "    ## Use probabalistic collocation method,\n",
      "    {name} = {val}\n",
      "    tensor_grid\n",
      "    ## with all cross-terms\n",
      "    basis_type total_order\n",
      "    \n",
      "    ## over-sampling ratio \n",
      "    collocation_ratio 3\n",
      "    ## how to compute coefficients\n",
      "    #least_angle_regression\n",
      "    #least_absolute_shrinkage\n",
      "    least_squares\n",
      "    \n",
      "    #cross_validation\n",
      "    \n",
      "    # post-processing\n",
      "    num_probability_levels = 11\n",
      "    probability_levels = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95\n",
      "    cumulative distribution\n",
      "    \n",
      "\"\"\"\n",
      "\n",
      "param_name = \"expansion_order\"\n",
      "param_vals = [3, 4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the experiment setup."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pickle.dump(dict(exp_name=exp_name, param_name=param_name, \n",
      "                 param_vals=param_vals, variables=variables,\n",
      "                 responses=responses, function_name=function_name,\n",
      "                 directive_base=pce_directive),\n",
      "            open(\"%s_exp.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Generate the model python and DAKOTA driver scripts and run DAKOTA\n",
      "\n",
      "These scripts will initially reside in the main directory, but will be copied to the `save` archive timestamped folder for referal later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"iterating parameter {name} over vals {vals}\".\\\n",
      "       format(name=param_name, vals=param_vals)\n",
      "\n",
      "results_dict = {}\n",
      "for param_val in param_vals:\n",
      "    print \"   \",  param_val\n",
      "\n",
      "    directive = pce_directive.format(name=param_name, val=param_val)\n",
      "\n",
      "    pickle.dump(dict(variables=variables, responses=responses,\n",
      "                     pce_directive=directive, imports=imports,\n",
      "                     function_name=function_name,\n",
      "                     function=function, graphics=graphics,\n",
      "                     parallel=PARALLEL),\n",
      "                open('config.p', 'wb'))\n",
      "\n",
      "    %run gen_scripts.py\n",
      "    !chmod +x model_run.py\n",
      "\n",
      "    foldername = !(date +%Y%m%d_%H%M%S)\n",
      "\n",
      "    foldername = foldername[0]\n",
      "    print foldername\n",
      "    results_dict[\"%s_%r\" % (param_name, param_val)] = foldername\n",
      "\n",
      "    print \"Now executing simulation with DAKOTA...\",\n",
      "    if USE_MPI:\n",
      "        !mpiexec -np 10 dakota -i model_pce.in > model_pce.out\n",
      "    else:\n",
      "        !dakota -i model_pce.in > model_pce.out\n",
      "    print \"complete.\\n\"\n",
      "\n",
      "    print \"Copying files...\"\n",
      "    save_dir = os.path.join(\"save\", foldername)\n",
      "    os.mkdir(save_dir)\n",
      "\n",
      "    for fn in [\"model_run.py\", \n",
      "               \"model_pce.in\", \"model_pce.out\", \"model_pce.dat\",\n",
      "               \"dakota.rst\", \"config.p\"]:\n",
      "        shutil.copy2(fn, save_dir)\n",
      "        print fn, \"->\", save_dir\n",
      "\n",
      "    print \"... done.\"\n",
      "    \n",
      "    time.sleep(1) # to ensure we don't over-write folders!\n",
      "    \n",
      "pickle.dump(results_dict, open(\"%s_results.dict\" % exp_name, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "iterating parameter expansion_order over vals [2, 3, 4]\n",
        "    2\n",
        "Writing model_run.py with model_run_script.template ...\n",
        "Writing model_pce.in with model_pce_parallel.template ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "20140926_103939\n",
        "Now executing simulation with DAKOTA..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r\n",
        "Warning: analysis_concurrency specification greater than length of\r\n",
        "\tanalysis_drivers list.  Truncating analysis_concurrency to 1.\r\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Process the output of all the runs\n",
      "results_dict = pickle.load(open(\"%s_results.dict\" % exp_name, 'rb'))\n",
      "\n",
      "for run_name, foldername in results_dict.iteritems():\n",
      "    print run_name, foldername\n",
      "    %run process_output.py {\"save/%s\" % foldername}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}