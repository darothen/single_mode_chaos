{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the steps necessary to post-process the set of chaos expansion experiments performed here. It will do a few things:\n",
    "\n",
    "- Archive all the scripts and intermediate output files for storage/backup elsewhere\n",
    "\n",
    "`version: April 22, 2015`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, pickle, pprint, shutil\n",
    "from collections import namedtuple, OrderedDict\n",
    "from itertools import product\n",
    "from zipfile import ZipFile, ZIP_DEFLATED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up some \"big picture\" information, such as the names of the experiments and any useful helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SAVE_DIR = os.path.join(os.getcwd(), \"save\")\n",
    "EXP_NAMES = [\"SM_OLS\", \"SM_LARS\", \"SM_LASSO\"]\n",
    "ARCH_NAME = \"SM_experiment.zip\"\n",
    "SAMPLE_FN = \"sample.csv\"\n",
    "\n",
    "def load_exp(name, sample=\"LHS\", archived=False):\n",
    "    \"\"\" Load an experiment's dictionary, results, LHS design, and\n",
    "    LHS results files \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    archived : boolean\n",
    "        Look for archived scheme first and fallback on original copy\n",
    "        if not available.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not archived:\n",
    "\n",
    "        with open(\"%s_exp.dict\" % name, 'r') as f:\n",
    "            exp_dict = pickle.load(f)\n",
    "        with open(\"%s_results.dict\" % name, 'r') as f:\n",
    "            results_dict = pickle.load(f)\n",
    "\n",
    "        design_name = \"%s_%s_design\" % (name, sample)\n",
    "        sample_design = pd.read_csv(\"%s.csv\" % design_name,\n",
    "                                    index_col=0)\n",
    "        sample_results = pd.read_csv(\"%s_results.csv\" % design_name,\n",
    "                                     index_col=0)\n",
    "        ## Rename Neq_ARG/MBN to Nderiv_ARG/MBN\n",
    "        for bad_key in [\"Neq_ARG\", \"Neq_MBN\"]:\n",
    "            print \"Re-named bad key (%s)\" % bad_key\n",
    "            if bad_key in sample_results:\n",
    "                sample_results.rename(\n",
    "                    columns={ bad_key: bad_key.replace(\"Neq\",\"Nderiv\") },\n",
    "                    inplace=True\n",
    "                )\n",
    "        \n",
    "        sampling_results = pd.concat([sample_design, sample_results], axis=1)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        with open(\"%s.p\" % name, \"r\") as f:\n",
    "            exp_dict = pickle.load(f)\n",
    "        results_dict = {}\n",
    "        \n",
    "        sample_fn = os.path.join(name, SAMPLE_FN)\n",
    "        sampling_results = pd.read_csv(sample_fn)\n",
    "        \n",
    "    return exp_dict, results_dict, sampling_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we should generate the datastore for running the pcm parameterization from Python and Fortran, which is accomplished by the script `pcm_param.py` run from the command line. Global var `RUNS` in this file contains mappings of experiment names and orders, and for each combination, will produce a file `{EXP_NAME}_{ORDER}.ascii`, which is what the Fortran code looks for. Additionally, it will create and HDF5 Datastore object on disk which retains all the details of the chaos expansions for quickly calling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM_LASSO\n",
      "    expansion_order_2\n",
      "    expansion_order_3\n",
      "    expansion_order_4\n",
      "    expansion_order_5\n",
      "SM_LARS\n",
      "    expansion_order_2\n",
      "    expansion_order_3\n",
      "    expansion_order_4\n",
      "    expansion_order_5\n",
      "SM_OLS\n",
      "    expansion_order_2\n",
      "    expansion_order_3\n",
      "    expansion_order_4\n",
      "    expansion_order_5\n",
      "V 1.0\n",
      "PCM (0.002096470910337105, 490.28014574731134, 0.57680017146742513)\n",
      "ARG (0.0020518378787395715, [483.37876924093644], [0.56868090498933699])\n",
      "MBN (0.0023060400580614806, [520.34672851897585], [0.61217262178703047])\n",
      "------------------------------------------------------------\n",
      "Locked out\n"
     ]
    }
   ],
   "source": [
    "%run pcm_param.py\n",
    "%run pcm_param.py --TEST --BLOCK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to move those files generated above into the appropriate directory.\n",
    "\n",
    "---\n",
    "\n",
    "Save the results by first organizing into a reliable folder stucture:\n",
    "\n",
    "    | this_dir/\n",
    "    | ------->/exp1.p\n",
    "    | ------->/--->\n",
    "    | ------->/--->/expansion_order_1\n",
    "    | ...\n",
    "    | ------->/--->/expansion_order_n\n",
    "    | ------->/--->/exp_1.ASCII\n",
    "    | ...\n",
    "    | ------->/--->/exp_n.ASCII\n",
    "    | ------->/--->/sample.csv\n",
    "    \n",
    "Note that they will be re-organized so that the raw output for each expansion is easily identifiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting experiment data\n",
      "   SM_OLS\n",
      "Re-named bad key (Neq_ARG)\n",
      "Re-named bad key (Neq_MBN)\n",
      "   Copying results set\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_OLS/expansion_order_2\n",
      "      SM_OLS_2.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_OLS/expansion_order_3\n",
      "      SM_OLS_3.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_OLS/expansion_order_4\n",
      "      SM_OLS_4.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_OLS/expansion_order_5\n",
      "      SM_OLS_5.ascii\n",
      "   SM_LARS\n",
      "Re-named bad key (Neq_ARG)\n",
      "Re-named bad key (Neq_MBN)\n",
      "   Copying results set\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LARS/expansion_order_2\n",
      "      SM_LARS_2.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LARS/expansion_order_3\n",
      "      SM_LARS_3.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LARS/expansion_order_4\n",
      "      SM_LARS_4.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LARS/expansion_order_5\n",
      "      SM_LARS_5.ascii\n",
      "      appending Smax_LARS_2\n",
      "      appending Nderiv_LARS_2\n",
      "      appending Neq_LARS_2\n",
      "      appending Nkn_LARS_2\n",
      "      appending Smax_LARS_3\n",
      "      appending Nderiv_LARS_3\n",
      "      appending Neq_LARS_3\n",
      "      appending Nkn_LARS_3\n",
      "      appending Smax_LARS_4\n",
      "      appending Nderiv_LARS_4\n",
      "      appending Neq_LARS_4\n",
      "      appending Nkn_LARS_4\n",
      "      appending Smax_LARS_5\n",
      "      appending Nderiv_LARS_5\n",
      "      appending Neq_LARS_5\n",
      "      appending Nkn_LARS_5\n",
      "   SM_LASSO\n",
      "Re-named bad key (Neq_ARG)\n",
      "Re-named bad key (Neq_MBN)\n",
      "   Copying results set\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LASSO/expansion_order_2\n",
      "      SM_LASSO_2.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LASSO/expansion_order_3\n",
      "      SM_LASSO_3.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LASSO/expansion_order_4\n",
      "      SM_LASSO_4.ascii\n",
      "      Overwriting existing /home/darothen/workspace/CESM_PCE_exp/8_pce_dakota/single_mode/SM_LASSO/expansion_order_5\n",
      "      SM_LASSO_5.ascii\n",
      "      appending Smax_LASSO_2\n",
      "      appending Nderiv_LASSO_2\n",
      "      appending Neq_LASSO_2\n",
      "      appending Nkn_LASSO_2\n",
      "      appending Smax_LASSO_3\n",
      "      appending Nderiv_LASSO_3\n",
      "      appending Neq_LASSO_3\n",
      "      appending Nkn_LASSO_3\n",
      "      appending Smax_LASSO_4\n",
      "      appending Nderiv_LASSO_4\n",
      "      appending Neq_LASSO_4\n",
      "      appending Nkn_LASSO_4\n",
      "      appending Smax_LASSO_5\n",
      "      appending Nderiv_LASSO_5\n",
      "      appending Neq_LASSO_5\n",
      "      appending Nkn_LASSO_5\n",
      "..done\n"
     ]
    }
   ],
   "source": [
    "experiments = {}\n",
    "\n",
    "print \"Collecting experiment data\"\n",
    "\n",
    "all_sampling_results = None\n",
    "\n",
    "for i, exp_name in enumerate(EXP_NAMES):\n",
    "    _, method = exp_name.split(\"_\")\n",
    "\n",
    "    print \"   \" + exp_name\n",
    "    exp_dir = os.path.join(os.getcwd(), exp_name)\n",
    "    if not os.path.exists(exp_dir):\n",
    "        os.mkdir(exp_dir)\n",
    "\n",
    "    exp, results, sampling_results = load_exp(exp_name)\n",
    "\n",
    "    sample_file = os.path.join(exp_dir, SAMPLE_FN)\n",
    "    sampling_results.to_csv(sample_file, mode='w')\n",
    "\n",
    "    print \"   Copying results set\"\n",
    "    for name, val in product([exp['param_name'], ], exp['param_vals']):\n",
    "        expansion = name + \"_\" + str(val)\n",
    "        \n",
    "        # Re-name columns in sampling_results for concatting to master\n",
    "        # file; map \"expansion_order\" -> \"OLS\", e.g.\n",
    "        cols = [ c for c in sampling_results.columns \n",
    "                 if c.endswith(expansion) ]\n",
    "        sampling_results.rename(\n",
    "            columns={ c: c.replace(name, method) for c in cols },\n",
    "            inplace=True\n",
    "        )\n",
    "\n",
    "        data_loc = os.path.join(SAVE_DIR, results[expansion])\n",
    "        save_loc = os.path.join(exp_dir, expansion)\n",
    "\n",
    "        # Copy to top-level dir for archiving here\n",
    "        if os.path.exists(save_loc):\n",
    "            print \"      Overwriting existing\", save_loc\n",
    "            shutil.rmtree(save_loc)\n",
    "        shutil.copytree(data_loc, save_loc)\n",
    "        \n",
    "        ## ASCII file\n",
    "        ASCII_fn = \"%s_%d.ascii\" % (exp_name, val)\n",
    "        print \"      \" + ASCII_fn\n",
    "        shutil.copy2(ASCII_fn, \n",
    "                     os.path.join(save_loc, ASCII_fn))\n",
    "    \n",
    "    # Join into master sampling results file\n",
    "    #pprint.pprint(sorted(sampling_results.columns))\n",
    "    if all_sampling_results is None:\n",
    "        all_sampling_results = sampling_results.copy()\n",
    "    else:\n",
    "        for col in sampling_results:\n",
    "            if not (col in all_sampling_results):\n",
    "                print \"      appending\", col\n",
    "                a = all_sampling_results.join(sampling_results[col])\n",
    "                all_sampling_results = a\n",
    "            \n",
    "    experiments[exp_name] = exp\n",
    "\n",
    "all_sampling_results.to_csv(\"SM_sampling_results.csv\")\n",
    "print \"..done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistics\n",
    "\n",
    "Compute summary statistics for different comparisons between the various chaos expansions and different predicted quantities. A few assumptions go into the output here:\n",
    "\n",
    "1. Compare different combinations of `Nderiv` from chaos expansions (using $S_\\text{max}$ to predict $N_d$) and $N_{eq}$, $N_{kn}$ from the parcel model. Record error statistics/metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SM_LASSO\n",
      "SM_LARS\n",
      "SM_OLS\n",
      "Writing...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from pce_vis import compute_stats, stat_label\n",
    "from functools import partial\n",
    "\n",
    "def result_key(output, name, val=0):\n",
    "    base_key = \"%s_%s\" % (output, name)\n",
    "    if val > 0:\n",
    "        base_key += \"_%d\" % val\n",
    "    return base_key\n",
    "\n",
    "def compute_stats_vs_parcel(df, output, name, val=0, output_parcel=None,\n",
    "                           power10=False):\n",
    "    if output_parcel is None:\n",
    "        output_parcel = output\n",
    "    \n",
    "    # Figure out which columns in df to pull\n",
    "    key_param = result_key(output, name, val)\n",
    "    key_parcel = result_key(output_parcel, \"parcel\")\n",
    "    \n",
    "    # Grab and clean data\n",
    "    data_df = pd.DataFrame({key_param: df[key_param],\n",
    "                            key_parcel: df[key_parcel]})\n",
    "    data_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    data_df.dropna(inplace=True)\n",
    "    \n",
    "    results = {}\n",
    "    results['log10'] = compute_stats(data_df[key_param], \n",
    "                                     data_df[key_parcel])   \n",
    "    data_df = 10.**(data_df)\n",
    "    results['normal'] = compute_stats(data_df[key_param], \n",
    "                                      data_df[key_parcel])   \n",
    "    \n",
    "    return results\n",
    "\n",
    "stats_df_mi = pd.MultiIndex.from_product(\n",
    "    [[\"Smax\", \"Neq\", \"Nkn\", \"Nderiv_Neq\", \"Nderiv_Nkn\"], \n",
    "     [\"log10\", \"normal\"],\n",
    "     [\"rmse\", \"nrmse\", \"mae\", \"r2\", \"mre\", \"mre_std\"]],\n",
    "    names=[\"result\", \"scaling\", \"stat\"]\n",
    ")\n",
    "\n",
    "all_stats = []\n",
    "for exp_name, exp in experiments.iteritems():\n",
    "    print exp_name\n",
    "    \n",
    "    _, method = exp_name.split(\"_\")\n",
    "    \n",
    "    for val in exp['param_vals']:\n",
    "        \n",
    "        stats = {}\n",
    "        \n",
    "        ## Smax\n",
    "        stats['Smax'] = compute_stats_vs_parcel(\n",
    "            all_sampling_results, \"Smax\", method, val\n",
    "        )\n",
    "        \n",
    "        ## Nderiv vs Neq\n",
    "        stats['Nderiv_Neq'] = compute_stats_vs_parcel(\n",
    "            all_sampling_results, \"Nderiv\", method, val, output_parcel=\"Neq\"\n",
    "        )\n",
    "        \n",
    "        ## Nderiv vs Nkn\n",
    "        stats['Nderiv_Nkn'] = compute_stats_vs_parcel(\n",
    "            all_sampling_results, \"Nderiv\", method, val, output_parcel=\"Nkn\"\n",
    "        )\n",
    "        \n",
    "        ## Neq vs Neq\n",
    "        stats['Neq'] = compute_stats_vs_parcel(\n",
    "            all_sampling_results, \"Neq\", method, val\n",
    "        )\n",
    "        \n",
    "        ## Nkn vs Nkn\n",
    "        stats['Nkn'] = compute_stats_vs_parcel(\n",
    "            all_sampling_results, \"Nkn\", method, val\n",
    "        )\n",
    "                \n",
    "        stats_vals = []\n",
    "        for key in stats_df_mi:\n",
    "            result, scaling, stat = key\n",
    "            stats_vals.append(stats[result][scaling][stat])\n",
    "        stats_df = pd.DataFrame(stats_vals, index=stats_df_mi,\n",
    "                                columns=[(method, val)], )\n",
    "        all_stats.append(stats_df.T)\n",
    "        \n",
    "pces_mi = pd.MultiIndex.from_tuples([df.index[0] for df in all_stats],\n",
    "                                     names=[\"method\", \"order\"])\n",
    "all_df = pd.concat(all_stats)\n",
    "all_df.set_index(pces_mi, inplace=True)\n",
    "\n",
    "print \"Writing...\"\n",
    "all_df.to_pickle(\"SM_stats.p\")\n",
    "print \"done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 'master dataset' with all the sampling data, tagged appropriately so that it can be split apart using pandas/seaborn to do factor analysis by PCE method, order, etc. This produces a \"tidy\" DataFrame in the style of Hadley Wickham; each row is one observation, with all metadata encoded to figure out where it came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS 2\n",
      "OLS 3\n",
      "OLS 4\n",
      "OLS 5\n",
      "LARS 2\n",
      "LARS 3\n",
      "LARS 4\n",
      "LARS 5\n",
      "LASSO 2\n",
      "LASSO 3\n",
      "LASSO 4\n",
      "LASSO 5\n",
      "ARG\n",
      "MBN\n",
      "Writing...\n"
     ]
    }
   ],
   "source": [
    "# Collect the parcel model output\n",
    "# Note - we've taken 10^x for all fields, because it's assumed that\n",
    "#        log10(x) is saved\n",
    "parcel_Smax = 10.**all_sampling_results['Smax_parcel']\n",
    "parcel_Neq = 10.**all_sampling_results['Neq_parcel']\n",
    "parcel_Nkn = 10.**all_sampling_results['Nkn_parcel']\n",
    "\n",
    "rel_errs_Smax, rel_errs_Neq, rel_errs_Nkn = [], [], []\n",
    "rel_errs_Nd_Neq, rel_errs_Nd_Nkn = [], []\n",
    "all_methods = []\n",
    "all_orders = []\n",
    "for exp_name in EXP_NAMES:\n",
    "    _, method = exp_name.split(\"_\")\n",
    "    for order in [ 2, 3, 4, 5 ]:\n",
    "        print method, order\n",
    "        key = \"%s_%d\" % (method, order)\n",
    "        Smaxes = 10.**all_sampling_results[\"Smax_\"+key]\n",
    "        Neqs = 10.**all_sampling_results[\"Neq_\"+key]\n",
    "        Nkns = 10.**all_sampling_results[\"Nkn_\"+key]\n",
    "        Nderivs = 10.**all_sampling_results[\"Nderiv_\"+key]\n",
    "        \n",
    "        n_tot = len(Smaxes)\n",
    "        \n",
    "        all_orders.extend([int(order), ]*n_tot)\n",
    "        all_methods.extend([method, ]*n_tot)\n",
    "        rel_errs_Smax.extend(100.*(Smaxes - parcel_Smax)/parcel_Smax)\n",
    "        rel_errs_Neq.extend(100.*(Neqs - parcel_Neq)/parcel_Neq)\n",
    "        rel_errs_Nkn.extend(100.*(Nkns - parcel_Nkn)/parcel_Nkn)\n",
    "        rel_errs_Nd_Neq.extend(100.*(Nderivs - parcel_Neq)/parcel_Neq)\n",
    "        rel_errs_Nd_Nkn.extend(100.*(Nderivs - parcel_Nkn)/parcel_Nkn)\n",
    "        \n",
    "for method in ['ARG', 'MBN']:\n",
    "    print method\n",
    "    Smaxes = 10.**all_sampling_results[\"Smax_\"+method]\n",
    "    Nderivs = 10.**all_sampling_results[\"Nderiv_\"+method]\n",
    "    \n",
    "    n_tot = len(Smaxes)\n",
    "        \n",
    "    all_orders.extend([0, ]*n_tot)\n",
    "    all_methods.extend([method, ]*n_tot)\n",
    "    rel_errs_Smax.extend(100.*(Smaxes - parcel_Smax)/parcel_Smax)\n",
    "    rel_errs_Nd_Neq.extend(100.*(Nderivs - parcel_Neq)/parcel_Neq)\n",
    "    rel_errs_Nd_Nkn.extend(100.*(Nderivs - parcel_Nkn)/parcel_Nkn)\n",
    "    rel_errs_Neq.extend([np.NaN, ]*n_tot)\n",
    "    rel_errs_Nkn.extend([np.NaN, ]*n_tot)\n",
    "    \n",
    "df = pd.DataFrame({'rel_err_Smax': rel_errs_Smax, \n",
    "                   'rel_err_Neq': rel_errs_Neq, \n",
    "                   'rel_err_Nkn': rel_errs_Nkn, \n",
    "                   'rel_err_Nd_Neq': rel_errs_Nd_Neq, \n",
    "                   'rel_err_Nd_Nkn': rel_errs_Nd_Nkn,\n",
    "                   'method': all_methods, 'order': all_orders})\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "print \"Writing...\"\n",
    "df.to_csv(\"SM_sampling_tidy.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress into a zip file for archival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SM_LASSO\n",
      "Re-named bad key (Neq_ARG)\n",
      "Re-named bad key (Neq_MBN)\n",
      "      compressing...\n",
      "      compressing...\n",
      "      compressing...\n",
      "      compressing...\n",
      "   Saving experiments dictionary\n",
      "\n",
      "   SM_LARS\n",
      "Re-named bad key (Neq_ARG)\n",
      "Re-named bad key (Neq_MBN)\n",
      "      compressing...\n",
      "      compressing...\n",
      "      compressing...\n",
      "      compressing...\n",
      "   Saving experiments dictionary\n",
      "\n",
      "   SM_OLS\n",
      "Re-named bad key (Neq_ARG)\n",
      "Re-named bad key (Neq_MBN)\n",
      "      compressing...\n",
      "      compressing...\n",
      "      compressing...\n",
      "      compressing...\n",
      "   Saving experiments dictionary\n",
      "\n",
      "   Saving combined sampling results and errors\n",
      "Archiving scripts\n",
      "   dakota_poly.py\n",
      "   dist_tools.py\n",
      "   pcm_param.py\n",
      "   pcm_param.f90\n",
      "   pcm_param.h5\n",
      "..done\n"
     ]
    }
   ],
   "source": [
    "with ZipFile(ARCH_NAME, 'w', compression=ZIP_DEFLATED) as zf:\n",
    "\n",
    "    for exp_name, exp in experiments.iteritems():\n",
    "        \n",
    "        print \"   \" + exp_name\n",
    "        exp_dir = os.path.join(os.getcwd(), exp_name)\n",
    "\n",
    "        exp, results, sampling_results = load_exp(exp_name)\n",
    "        \n",
    "        # Save sampling results into zipfile\n",
    "        sample_file = os.path.join(exp_dir, SAMPLE_FN)\n",
    "        zf.write(sample_file, \n",
    "                 \"%s/%s_sample_results.csv\" % (exp_name, exp_name))\n",
    "        \n",
    "        for name, val in product([exp['param_name'], ], exp['param_vals']):\n",
    "            expansion = name + \"_\" + str(val)\n",
    "\n",
    "            data_loc = os.path.join(SAVE_DIR, results[expansion])\n",
    "            save_loc = os.path.join(exp_dir, expansion)\n",
    "\n",
    "            # List and copy into zip file\n",
    "            print \"      compressing...\"\n",
    "            for f in os.listdir(save_loc):\n",
    "                zf.write(os.path.join(save_loc, f), \n",
    "                         \"%s/%s/%s\" % (exp_name, expansion, f))\n",
    "                \n",
    "            # ASCII file\n",
    "            ASCII_fn = \"%s_%d.ascii\" % (exp_name, val)\n",
    "            zf.write(ASCII_fn, \"%s/%s\" % (exp_name, ASCII_fn))\n",
    "                        \n",
    "\n",
    "        # Dictionary for this exp\n",
    "        print \"   Saving experiments dictionary\"\n",
    "        with open(\"%s.p\" % exp_name, 'w') as pf:\n",
    "            pickle.dump(exp, pf)\n",
    "        zf.write(\"%s.p\" % exp_name)\n",
    "        \n",
    "        print \"\"\n",
    "        \n",
    "    # All sampling results\n",
    "    print \"   Saving combined sampling results and errors\"\n",
    "    zf.write(\"SM_sampling_results.csv\")\n",
    "    zf.write(\"SM_sampling_tidy.csv\")\n",
    "    zf.write(\"SM_stats.p\")\n",
    "    \n",
    "    # archiving scripts\n",
    "    print \"Archiving scripts\"\n",
    "    for f in [\"dakota_poly.py\", \"dist_tools.py\", \n",
    "              'pcm_param.py', 'pcm_param.f90', 'pcm_param.h5']:\n",
    "        print \"   \" + f\n",
    "        zf.write(f)\n",
    "        \n",
    "    print \"..done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
